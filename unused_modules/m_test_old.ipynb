{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a472bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_comm import *\n",
    "from nccl_primitives import *\n",
    "from nsys_events import *\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# import aiofiles\n",
    "from collections import defaultdict\n",
    "import json\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1892734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_communicators(comm_info: dict):\n",
    "    communicators = {}\n",
    "    gpu_devices = {}\n",
    "    for comm_id, comm_data in comm_info.items():\n",
    "        curr_comm_gpus = []\n",
    "        for rank, rank_data in comm_data[\"rank_To_rankInfo\"].items():\n",
    "            gpu_id = rank_data[\"gpuId\"]\n",
    "            node_id = rank_data[\"goal_rank\"]\n",
    "            gpu_devices.setdefault(gpu_id, GPUDevice(gpu_id, node_id))\n",
    "            curr_comm_gpus.append((int(rank), gpu_devices[gpu_id]))\n",
    "        curr_comm_gpus.sort(key=lambda x: x[0])\n",
    "        communicators[comm_id] = Communicator(comm_id, [gpu for _, gpu in curr_comm_gpus])\n",
    "        for rank, rank_data in comm_data[\"rank_To_rankInfo\"].items():\n",
    "            gpu_id = rank_data[\"gpuId\"]\n",
    "            comm = communicators[comm_id]\n",
    "            chnl_info = rank_data[\"channel_info\"]\n",
    "            for ring_info in chnl_info[\"Ring\"]:\n",
    "                comm.add_ring_topo(\n",
    "                    int(rank), int(ring_info[\"previous_rank\"]), int(ring_info[\"next_rank\"])\n",
    "                )\n",
    "            for tree_info in chnl_info[\"Tree\"]:\n",
    "                children = [int(c) for c in (tree_info[f\"child_{i}_rank\"] for i in range(1, 4))]\n",
    "                comm.add_tree_topo(\n",
    "                    int(rank), int(tree_info[\"parent_rank\"]), [c for c in children if c >= 0]\n",
    "                )\n",
    "    return communicators, gpu_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3a57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nccl_example/example_allgather/results/nsys_events_intermediate_output.json\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    fields = text.split(\"\\n\\n\")\n",
    "    data = [json.loads(field) for field in fields if field.strip()]\n",
    "    HostName_To_GoalRank, Comm_Info, CUPTI_Kernel_Results, NCCL_Events, Comm_Init_Events = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03390b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "communicators, gpu_devices = construct_communicators(Comm_Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501b5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "collective_ops = { # make all collectives as AllReduce for testing\n",
    "    \"AllReduce\": AllReduce,\n",
    "    \"AllGather\": AllGather,\n",
    "    \"ReduceScatter\": ReduceScatter,\n",
    "    \"Broadcast\": Broadcast,\n",
    "    \"Reduce\": Reduce\n",
    "}\n",
    "algo_mapping = {\n",
    "    0: CollAlgo.TREE,\n",
    "    1: CollAlgo.RING,\n",
    "}\n",
    "proto_mapping = {\n",
    "    0: NCCLProto.LL,\n",
    "    1: NCCLProto.LL128,\n",
    "    2: NCCLProto.SIMPLE,\n",
    "}\n",
    "\n",
    "def construct_collectives(\n",
    "        gpu_devices: Dict[int, GPUDevice],\n",
    "        communicators: Dict[str, Communicator],\n",
    "        collectives: dict\n",
    "):\n",
    "    flattened_collectives = {}\n",
    "    for node_id, node_collectives in collectives.items():\n",
    "        for gpu_id, gpu_collectives in node_collectives.items():\n",
    "            flattened_collectives[int(gpu_id)] = gpu_collectives\n",
    "    \n",
    "    for gpu_id, gpu_collectives in flattened_collectives.items():\n",
    "        for stream_id, stream_collectives in gpu_collectives.items():\n",
    "            for coll in stream_collectives:\n",
    "                chnl_infos = []\n",
    "                for chnl_info in coll[\"elems\"]:\n",
    "                    chnl_infos.append(\n",
    "                        CollChnlInfo(\n",
    "                            3,\n",
    "                            chnl_info[\"count\"],\n",
    "                            chnl_info[\"chunkCount\"],\n",
    "                            chnl_info[\"workCount\"],\n",
    "                            chnl_info[\"lastChunkCount\"],\n",
    "                            chnl_info[\"workOffset\"],\n",
    "                            chnl_info[\"sendbuff\"],\n",
    "                            chnl_info[\"recvbuff\"]\n",
    "                        )\n",
    "                    )\n",
    "                coll_info = CollInfo(\n",
    "                    coll.get(\"root_rank\", 0),\n",
    "                    coll.get(\"redOp\", 0),\n",
    "                    algo_mapping[int(coll[\"algorithm\"])],\n",
    "                    proto_mapping[int(coll[\"protocol\"])],\n",
    "                    coll[\"data_size\"],\n",
    "                    coll[\"type_size\"],\n",
    "                    -1,\n",
    "                    -1,\n",
    "                    coll[\"chunkSteps\"],\n",
    "                    coll[\"sliceSteps\"],\n",
    "                    coll[\"stepSize\"]\n",
    "                )\n",
    "                collective = collective_ops[coll[\"event_type\"]](\n",
    "                    gpu_devices[gpu_id],\n",
    "                    communicators[coll[\"commId\"]],\n",
    "                    coll_info,\n",
    "                    chnl_infos,\n",
    "                    0\n",
    "                )\n",
    "                gpu_devices[gpu_id].add_collective(stream_id, collective, coll[\"ts_gpu_start\"], coll[\"ts_gpu_end\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53682602",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nccl_example/example_allgather/results/nsys_events_merged_output.json\", \"r\") as f:\n",
    "    coll_data = json.load(f)\n",
    "construct_collectives(gpu_devices, communicators, coll_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1bf5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc0b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data(\"npkit_benchmark_results/ault/npkit_data_summary_Simple.json\", \"npkit_benchmark_results/ault/npkit_data_summary_LL.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085925ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:writing goal file\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated goals for stream on GPU 0 with context -1 for 1 collectives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated goals for stream on GPU 1 with context -1 for 1 collectives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated goals for stream on GPU 2 with context -1 for 1 collectives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated goals for stream on GPU 3 with context -1 for 1 collectives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated goals for stream on GPU 4 with context -1 for 1 collectives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 188.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated goals for stream on GPU 5 with context -1 for 1 collectives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpu2goal_rank = {gpu: i for i, gpu in enumerate(g for g in gpu_devices.values() if len(g.streams) > 0)}\n",
    "\n",
    "with open(\"trace_allgather.goal\", \"w\") as f:\n",
    "    logger.info(\"writing goal file\")\n",
    "    gpus = [gpu for gpu in gpu_devices.values() if len(gpu.streams) > 0]\n",
    "    f.write(f\"num_ranks {len(gpus)}\\n\")\n",
    "    for gpu in tqdm(gpus):\n",
    "        f.write(f\"rank {gpu2goal_rank[gpu]} {{\\n\")\n",
    "        for line in gpu.generate_goal_lines(gpu2goal_rank, nic=0):\n",
    "            f.write(f\"{line}\\n\")\n",
    "        f.write(\"}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nccl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
